<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Pierre Ouannes</title>
    <link>https://pouannes.github.io/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Pierre Ouannes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 29 Mar 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://pouannes.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Understanding callbacks in fastai</title>
      <link>https://pouannes.github.io/blog/callbacks-fastai/</link>
      <pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pouannes.github.io/blog/callbacks-fastai/</guid>
      <description>fastai is a great library for Deep Learning with many powerful features, which make it very easy to quickly build state of the art models, but also to tweak them as you wish. One of the best features of fastai is its callbacks system that lets you customize simply pretty much everything.
However, it can take getting used to and that&amp;rsquo;s the purpose of this post: presenting the callback system in fastai, explaining how it works and how to use it and finally showing you a few examples.</description>
    </item>
    
    <item>
      <title>How to initialize deep neural networks? Xavier and Kaiming initialization</title>
      <link>https://pouannes.github.io/blog/initialization/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pouannes.github.io/blog/initialization/</guid>
      <description>Initialization of neural networks isn&amp;rsquo;t something we think a lot about nowadays. It&amp;rsquo;s all hidden behind the different Deep Learning frameworks we use, like TensorFlow or PyTorch. However, it&amp;rsquo;s at the heart of why and how we can make neural networks as deep as they are today, and it was a significant bottleneck just a few years ago.
In this post, I&amp;rsquo;ll walk over the initialization part of two very significant papers:</description>
    </item>
    
    <item>
      <title>Roadmap to ML</title>
      <link>https://pouannes.github.io/blog/roadmap-to-ml/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pouannes.github.io/blog/roadmap-to-ml/</guid>
      <description>This is my roadmap to go from an undergraduate CS and math background to a very good understanding of Machine Learning and Deep Learning concepts, and more importantly to successfully apply ML and DL. I will put here everything I&amp;rsquo;ve done (in italic), I&amp;rsquo;m doing (in bold), and I&amp;rsquo;m planning on doing (normal text) in order to reach that goal.
Courses  Andrew Ng&amp;rsquo;s Coursera Machine Learning course fast.ai course part 1 v3 Udacity Intro to deep learning with PyTorch fast.</description>
    </item>
    
  </channel>
</rss>