<!DOCTYPE html>
<html lang="en">
    
    


    <html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="0" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.53" />





<title>How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="How to initialize deep neural networks? Xavier and Kaiming initialization"/>
<meta name="twitter:description" content="Initialization of neural networks isn&rsquo;t something we think a lot about nowadays. It&rsquo;s all hidden behind the different Deep Learning frameworks we use, like TensorFlow or PyTorch. However, it&rsquo;s at the heart of why and how we can make neural networks as deep as they are today, and it was a significant bottleneck just a few years ago.
In this post, I&rsquo;ll walk over the initialization part of two very significant papers:"/>

<meta property="og:title" content="How to initialize deep neural networks? Xavier and Kaiming initialization" />
<meta property="og:description" content="Initialization of neural networks isn&rsquo;t something we think a lot about nowadays. It&rsquo;s all hidden behind the different Deep Learning frameworks we use, like TensorFlow or PyTorch. However, it&rsquo;s at the heart of why and how we can make neural networks as deep as they are today, and it was a significant bottleneck just a few years ago.
In this post, I&rsquo;ll walk over the initialization part of two very significant papers:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://pouannes.github.io/blog/initialization/" /><meta property="article:published_time" content="2019-03-22T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-03-22T00:00:00&#43;00:00"/><meta property="og:site_name" content="Title" />

    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.c89db144a840e340dad4257198a9a292a9ab52f3e6b19626c92d06088985455e.css" integrity="sha256-yJ2xRKhA40Da1CVxmKmikqmrUvPmsZYmyS0GCImFRV4=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">




<link rel="stylesheet" href="/scss/tocbot.51e1ec283fbfba143803cf9b8af1b615adc81bb90cee129a1a771719e5a60d83.css" integrity="sha256-UeHsKD&#43;/uhQ4A8&#43;bivG2Fa3IG7kM7hKaGncXGeWmDYM=">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://pouannes.github.io">Pierre Ouannes</a>
      </span>
      
      
      
      
      <div class="author-image">
        <img src="https://pouannes.github.io/img/profil.png" alt="Author Image" class="img--circle img--headshot element--center">
      </div>
      
      
      
      <p class="site__description">
        
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Pierre Ouannes</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/blog/">
						<span>Blog</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/about/">
						<span>About Me</span>
					</a>
				</li>
			 
		
		</li>
	</ul>
</div>

        <section class="social">
	
	<a href="https://twitter.com/PierreOuannes" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/pouannes" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	<a href="https://linkedin.com/in/pierre-ouannes" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://medium.com/@pierreouannes" rel="me"><i class="fab fa-medium fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
</section>

      </div>
      

<div class="builtwith">
Built with <a href="https://gohugo.io">Hugo</a> ❤️ <a href="https://github.com/htr3n/hyde-hyde">hyde-hyde</a>.
</div>


    </div>
  </div>

</div>
        <div class="content container">
            
    <article>
  <header>
    <h1>How to initialize deep neural networks? Xavier and Kaiming initialization</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Mar 22, 2019
    
    
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/deep-learning">deep learning</a>
           
      
          <a class="badge badge-tag" href="/tags/paper-review">paper review</a>
          
      
    
    
    
        <br/>
        <i class="fas fa-clock"></i> 17 min read
    
</div>


  </header>
  
  
    
    <div class="toc-wrapper">
      <input type="checkbox" id="tocToggle">
      <label for="tocToggle">Table of Content</label>
      
        <div class="toc" id="TableOfContents"></div>
      
    </div>
    
  
  <div class="post">
    

<p>Initialization of neural networks isn&rsquo;t something we think a lot about nowadays. It&rsquo;s all hidden behind the different Deep Learning frameworks we use, like TensorFlow or PyTorch. However, it&rsquo;s at the heart of why and how we can make neural networks as deep as they are today, and it was a significant bottleneck just a few years ago.</p>

<p>In this post, I&rsquo;ll walk over the initialization part of two very significant papers:</p>

<ul>
<li><p><a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank">Understanding the difficulty of training deep feedforward neural networks</a>:  the 2010 paper that introduced &lsquo;Xavier initialization&rsquo;,</p></li>

<li><p><a href="https://arxiv.org/abs/1502.01852" target="_blank">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>: winner of the 2015 ImageNet challenge, the paper that adapted Xavier initialization into &lsquo;Kaiming initialization&rsquo;.</p></li>
</ul>

<p>To ease reading, I&rsquo;ll be referencing to the two papers by &lsquo;the Xavier paper&rsquo;, the &lsquo;Kaiming paper&rsquo;, sometimes even only by &lsquo;Xavier&rsquo; and &lsquo;Kaiming&rsquo;.</p>

<p>First, let&rsquo;s setup the problem and understand why initialization is so important.</p>

<p><em>Important note: there&rsquo;s quite a bit of LaTeX formulas in this post that need JavaScript to render. It might take a bit of time to load everything properly, and obviously won&rsquo;t if you deactivated JS.</em></p>

<h2 id="the-initialization-headache">The initialization headache</h2>

<p>Why is initialization essential to deep networks? It turns out that if you do it wrong, it can lead to exploding or vanishing weights and gradients. That means that either the weights of the model explode to infinity, or they vanish to 0 (literally, because computers can&rsquo;t represent infinitely accurate floating point numbers), which make training deep neural networks very challenging. And the deeper the network, the harder it becomes to keep the weights at reasonable values. We&rsquo;ll see why that&rsquo;s the case in the following sections.</p>

<p>The Xavier paper worked with neural networks with (only!) 5 hidden layers, but the effect is already very visible if no care is given to initialization.</p>

<p>Here&rsquo;s the histogram of the mean values of the activations:</p>

<p><img src="/initialization/activation_std.png#center" alt="activ std init" /></p>

<p align="center"><i> Activation values with standard initialization, <a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank"> source: Xavier paper </a> </i></p>

<p>And here&rsquo;s the histogram of the mean values of the backpropagated gradients:</p>

<p><img src="/initialization/back_prop_grad_std.png#center" alt="grad std init" /></p>

<p align="center"><i> Gradient values with standard initialization, <a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank"> source: Xavier paper </a> </i></p>

<p>As you can see, in this case the activation values tend to vanish, and the gradients are also vanishing (yes, vanishing, because remember they are computed backwards: from layer 5 to layer 1). And that&rsquo;s only with 5 hidden layers! It&rsquo;s clear we need a better way of initializing the network, that doesn&rsquo;t lead to exploding or vanishing activations and gradients. That&rsquo;s (one of) the contribution of Xavier initialization, that was later refined into Kaiming initialization.</p>

<h2 id="xavier-and-kaiming-initialization">Xavier and Kaiming initialization</h2>

<p>The Xavier and Kaiming papers follow a very similar reasoning, that differs a tiny bit at the end. The only difference is that the Kaiming paper takes into account the activation function, whereas Xavier does not (or rather, Xavier approximates the derivative at 0 of the activation function by 1). Most of the math is adapted from the Kaiming paper, because I find it simpler and clearer. The main difference between this post and the papers is that this post is more detailed, so – hopefully – clearer to the uninitiated. It&rsquo;s still a bit mathy though, no way around that.</p>

<p>Anyway, let&rsquo;s get into it.</p>

<p>We&rsquo;ll study first the forward-propagation case, then the backward one. They are quite similar but still sufficiently different that we need to do both. Then we&rsquo;ll see if and how we have to take those differences into account. Finally, we&rsquo;ll take a look at how better behaved deep networks become with Xavier and Kaiming initializations.</p>

<p><em>Note: when I reference an equation number, it&rsquo;s clickable.</em></p>

<h3 id="forward-propagation">Forward-propagation</h3>

<p>For each layer $l$, the response is written as:</p>


<span class="jsonly">
   
      $$\begin{align} \boldsymbol{y_l} = W_l \boldsymbol{x_l} &#43; \boldsymbol{b_l} \label{eq:fwd} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20%5cboldsymbol%7by_l%7d%20%3d%20W_l%20%5cboldsymbol%7bx_l%7d%20%2b%20%5cboldsymbol%7bb_l%7d%20%5clabel%7beq%3afwd%7d%20%5cend%7balign%7d" title="\begin{align} \boldsymbol{y_l} = W_l \boldsymbol{x_l} &#43; \boldsymbol{b_l} \label{eq:fwd} \end{align}" />
      </div>
  
</noscript>

<p>Do you notice that $\boldsymbol{y_l}, \boldsymbol{x_l}$ and $\boldsymbol{b_l}$ are in bold? That means those are vectors. I know, it can be confusing. It&rsquo;s to make you pay attention! Or so my teachers keep repeating to me&hellip; Anyway, here&rsquo;s the definition of each term of this equation:</p>

<ul>
<li><p>$\boldsymbol{x_l}$ is a $ n_l\text{-by-}1 $ vector that represents the activations of the previous layer 
<span class="jsonly">
                
      \(\boldsymbol{y_{l-1}}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cboldsymbol%7by_%7bl-1%7d%7d" title="\boldsymbol{y_{l-1}}" />
  
</noscript> that were passed through the activation function 
<span class="jsonly">
                
      \(f\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;f" title="f" />
  
</noscript>, so we have

<span class="jsonly">
   
      $$\boldsymbol{x_l} = f(\boldsymbol{y_{l-1}})$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cboldsymbol%7bx_l%7d%20%3d%20f%28%5cboldsymbol%7by_%7bl-1%7d%7d%29" title="\boldsymbol{x_l} = f(\boldsymbol{y_{l-1}})" />
      </div>
  
</noscript>

<span class="jsonly">
                
      \(n_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;n_l" title="n_l" />
  
</noscript> is the number of activations of layer 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript>.</p></li>

<li><p>
<span class="jsonly">
                
      \(W_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;W_l" title="W_l" />
  
</noscript> is a 
<span class="jsonly">
                
      \(d_l\text{-by-}n_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;d_l%5ctext%7b-by-%7dn_l" title="d_l\text{-by-}n_l" />
  
</noscript> matrix of all the connections from layer 
<span class="jsonly">
                
      \(l-1\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l-1" title="l-1" />
  
</noscript> to layer 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript>, with 
<span class="jsonly">
                
      \(d_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;d_l" title="d_l" />
  
</noscript> the number of filters of the convolutional layer (or equivalently the number of channels).</p></li>

<li><p>
<span class="jsonly">
                
      \(\boldsymbol{b_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cboldsymbol%7bb_l%7d" title="\boldsymbol{b_l}" />
  
</noscript> is the vector of biases of layer 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript>  (that are usually initialized to 0).</p></li>

<li><p>
<span class="jsonly">
                
      \(\boldsymbol{y_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cboldsymbol%7by_l%7d" title="\boldsymbol{y_l}" />
  
</noscript>, as mentioned before, is the vector of activations of layer 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript> before they go though the activation function.</p></li>
</ul>

<p>A few hypotheses are made on those vectors:</p>

<ul>
<li><p>The initialized elements in 
<span class="jsonly">
                
      \(W_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;W_l" title="W_l" />
  
</noscript> are mutually independent and share the same distribution.</p></li>

<li><p>Likewise, the elements in 
<span class="jsonly">
                
      \(\boldsymbol{x_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cboldsymbol%7bx_l%7d" title="\boldsymbol{x_l}" />
  
</noscript> are mutually independent and share the same distribution.</p></li>

<li><p>
<span class="jsonly">
                
      \(\boldsymbol{x_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cboldsymbol%7bx_l%7d" title="\boldsymbol{x_l}" />
  
</noscript> and 
<span class="jsonly">
                
      \(W_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;W_l" title="W_l" />
  
</noscript> are independent of each other.</p></li>
</ul>

<p>Under those assumptions, if we take the variance of equation 
<span class="jsonly">
                
      \((\ref{eq:fwd})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd%7d%29" title="(\ref{eq:fwd})" />
  
</noscript>  we get</p>


<span class="jsonly">
   
      $$\begin{align} Var[y_l] = n_l Var[w_lx_l] \label{eq:fwd_var} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20Var%5by_l%5d%20%3d%20n_l%20Var%5bw_lx_l%5d%20%5clabel%7beq%3afwd_var%7d%20%5cend%7balign%7d" title="\begin{align} Var[y_l] = n_l Var[w_lx_l] \label{eq:fwd_var} \end{align}" />
      </div>
  
</noscript>

<p>Wait, what? Where did the bold vectors go? And what about the bias? And wasn&rsquo;t that 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript>  a bit bigger in equation 
<span class="jsonly">
                
      \((\ref{eq:fwd})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd%7d%29" title="(\ref{eq:fwd})" />
  
</noscript>? What&rsquo;s 
<span class="jsonly">
                
      \(n_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;n_l" title="n_l" />
  
</noscript> doing here? Acute observations!</p>

<p>Explaining where that result comes from will take a bit of math. If you don&rsquo;t speak that language (or if you are feeling confident in me, or at least in Kaiming and his co-authors), feel free to skip to equation 
<span class="jsonly">
                
      \((\ref{eq:fwd_var2})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_var2%7d%29" title="(\ref{eq:fwd_var2})" />
  
</noscript>, you&rsquo;re not missing out on a lot.</p>

<p>Let 
<span class="jsonly">
                
      \(W_l = (w_{i,j})_{1 \leq i \leq d_l, \, 1 \leq j \leq n_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;W_l%20%3d%20%28w_%7bi%2cj%7d%29_%7b1%20%5cleq%20i%20%5cleq%20d_l%2c%20%5c%2c%201%20%5cleq%20j%20%5cleq%20n_l%7d" title="W_l = (w_{i,j})_{1 \leq i \leq d_l, \, 1 \leq j \leq n_l}" />
  
</noscript>, with all 
<span class="jsonly">
                
      \(w_{i,j}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_%7bi%2cj%7d" title="w_{i,j}" />
  
</noscript> following the same distribution, as mentioned above. Similarly, let 
<span class="jsonly">
                
      \(\boldsymbol{x_l} = (x_j)_{1 \leq j \leq n_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cboldsymbol%7bx_l%7d%20%3d%20%28x_j%29_%7b1%20%5cleq%20j%20%5cleq%20n_l%7d" title="\boldsymbol{x_l} = (x_j)_{1 \leq j \leq n_l}" />
  
</noscript>, with all 
<span class="jsonly">
                
      \(x_j\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_j" title="x_j" />
  
</noscript> following the same distribution. We have:</p>


<span class="jsonly">
   
      $$W_l\boldsymbol{x_l} = \begin{bmatrix} \sum\limits_{j=1}^{n_l} w_{1,j}x_j \\ \sum\limits_{j=1}^{n_l} w_{2,j}x_j  \\ \vdots \\ \sum\limits_{j=1}^{n_l} w_{d,j}x_j  \end{bmatrix}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?W_l%5cboldsymbol%7bx_l%7d%20%3d%20%5cbegin%7bbmatrix%7d%20%5csum%5climits_%7bj%3d1%7d%5e%7bn_l%7d%20w_%7b1%2cj%7dx_j%20%5c%5c%20%5csum%5climits_%7bj%3d1%7d%5e%7bn_l%7d%20w_%7b2%2cj%7dx_j%20%20%5c%5c%20%5cvdots%20%5c%5c%20%5csum%5climits_%7bj%3d1%7d%5e%7bn_l%7d%20w_%7bd%2cj%7dx_j%20%20%5cend%7bbmatrix%7d" title="W_l\boldsymbol{x_l} = \begin{bmatrix} \sum\limits_{j=1}^{n_l} w_{1,j}x_j \\ \sum\limits_{j=1}^{n_l} w_{2,j}x_j  \\ \vdots \\ \sum\limits_{j=1}^{n_l} w_{d,j}x_j  \end{bmatrix}" />
      </div>
  
</noscript>

<p>We then take the variance of that expression; the right handside is now equal to the trace of the variance-covariance matrix. Only the diagonal is not equal to zero: because of the independence of the variables, the covariances are null:</p>


<span class="jsonly">
   
      $$\begin{align*} Var[W_l\boldsymbol{x_l}] &amp;= Tr \left( \quad \begin{bmatrix} Var\left[\sum\limits_{j=1}^{n_l} w_{1,j}x_j\right] &amp; 0 &amp; \cdots &amp; 0\\ 0 &amp; Var\left[\sum\limits_{j=1}^{n_l} w_{2,j}x_j\right] &amp; \cdots &amp; 0 \\ \vdots &amp; &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; Var\left[\sum\limits_{j=1}^{n_l} w_{d,j}x_j\right] \end{bmatrix} \quad  \right) \\ \\ &amp;= \sum_{k = 1}^{d_l} Var\left[\sum_{j=1}^{n_l} w_{k, j} x_j\right] \end{align*}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%2a%7d%20Var%5bW_l%5cboldsymbol%7bx_l%7d%5d%20%26%3d%20Tr%20%5cleft%28%20%5cquad%20%5cbegin%7bbmatrix%7d%20Var%5cleft%5b%5csum%5climits_%7bj%3d1%7d%5e%7bn_l%7d%20w_%7b1%2cj%7dx_j%5cright%5d%20%26%200%20%26%20%5ccdots%20%26%200%5c%5c%200%20%26%20Var%5cleft%5b%5csum%5climits_%7bj%3d1%7d%5e%7bn_l%7d%20w_%7b2%2cj%7dx_j%5cright%5d%20%26%20%5ccdots%20%26%200%20%5c%5c%20%5cvdots%20%26%20%26%20%5cddots%20%26%20%5cvdots%20%5c%5c%200%20%26%20%5ccdots%20%26%200%20%26%20Var%5cleft%5b%5csum%5climits_%7bj%3d1%7d%5e%7bn_l%7d%20w_%7bd%2cj%7dx_j%5cright%5d%20%5cend%7bbmatrix%7d%20%5cquad%20%20%5cright%29%20%5c%5c%20%5c%5c%20%26%3d%20%5csum_%7bk%20%3d%201%7d%5e%7bd_l%7d%20Var%5cleft%5b%5csum_%7bj%3d1%7d%5e%7bn_l%7d%20w_%7bk%2c%20j%7d%20x_j%5cright%5d%20%5cend%7balign%2a%7d" title="\begin{align*} Var[W_l\boldsymbol{x_l}] &amp;= Tr \left( \quad \begin{bmatrix} Var\left[\sum\limits_{j=1}^{n_l} w_{1,j}x_j\right] &amp; 0 &amp; \cdots &amp; 0\\ 0 &amp; Var\left[\sum\limits_{j=1}^{n_l} w_{2,j}x_j\right] &amp; \cdots &amp; 0 \\ \vdots &amp; &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; Var\left[\sum\limits_{j=1}^{n_l} w_{d,j}x_j\right] \end{bmatrix} \quad  \right) \\ \\ &amp;= \sum_{k = 1}^{d_l} Var\left[\sum_{j=1}^{n_l} w_{k, j} x_j\right] \end{align*}" />
      </div>
  
</noscript>

<p>But because all the 
<span class="jsonly">
                
      \(w_{i,j}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_%7bi%2cj%7d" title="w_{i,j}" />
  
</noscript> and the 
<span class="jsonly">
                
      \(x_i\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_i" title="x_i" />
  
</noscript> are independent from each other, the variance of the sum is the sum of the variances. Thus:</p>


<span class="jsonly">
   
      $$Var[W_l\boldsymbol{x_l}] = \sum_{k = 1}^{d_l} \sum_{j=1}^{n_l} Var\left[w_{k, j} x_j\right] $$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?Var%5bW_l%5cboldsymbol%7bx_l%7d%5d%20%3d%20%5csum_%7bk%20%3d%201%7d%5e%7bd_l%7d%20%5csum_%7bj%3d1%7d%5e%7bn_l%7d%20Var%5cleft%5bw_%7bk%2c%20j%7d%20x_j%5cright%5d%20" title="Var[W_l\boldsymbol{x_l}] = \sum_{k = 1}^{d_l} \sum_{j=1}^{n_l} Var\left[w_{k, j} x_j\right] " />
      </div>
  
</noscript>

<p>And because all the 
<span class="jsonly">
                
      \(w_{i,j}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_%7bi%2cj%7d" title="w_{i,j}" />
  
</noscript>  and 
<span class="jsonly">
                
      \(x_i\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_i" title="x_i" />
  
</noscript> follow the same distribution (respectively), all the variances are equal to a common variance we note 
<span class="jsonly">
                
      \(Var[w_l x_l]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;Var%5bw_l%20x_l%5d" title="Var[w_l x_l]" />
  
</noscript>. Hence:</p>


<span class="jsonly">
   
      $$\begin{align*}Var[W_l\boldsymbol{x_l}] &amp;= \sum_{k = 1}^{d_l} \sum_{j=1}^{n_l} Var\left[w_{l} x_l\right] \\ \\ &amp;= d_l \times n_l \times Var\left[w_{l} x_l\right] \end{align*}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%2a%7dVar%5bW_l%5cboldsymbol%7bx_l%7d%5d%20%26%3d%20%5csum_%7bk%20%3d%201%7d%5e%7bd_l%7d%20%5csum_%7bj%3d1%7d%5e%7bn_l%7d%20Var%5cleft%5bw_%7bl%7d%20x_l%5cright%5d%20%5c%5c%20%5c%5c%20%26%3d%20d_l%20%5ctimes%20n_l%20%5ctimes%20Var%5cleft%5bw_%7bl%7d%20x_l%5cright%5d%20%5cend%7balign%2a%7d" title="\begin{align*}Var[W_l\boldsymbol{x_l}] &amp;= \sum_{k = 1}^{d_l} \sum_{j=1}^{n_l} Var\left[w_{l} x_l\right] \\ \\ &amp;= d_l \times n_l \times Var\left[w_{l} x_l\right] \end{align*}" />
      </div>
  
</noscript>

<p>Following the exact same reasoning, albeit more simply because there&rsquo;s no product, we obtain:</p>


<span class="jsonly">
   
      $$Var[\boldsymbol{y_l}] = d_l \times Var[y_l]$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?Var%5b%5cboldsymbol%7by_l%7d%5d%20%3d%20d_l%20%5ctimes%20Var%5by_l%5d" title="Var[\boldsymbol{y_l}] = d_l \times Var[y_l]" />
      </div>
  
</noscript>

<p>As I said, the biases are initialized to 0 thus 
<span class="jsonly">
                
      \(Var[\boldsymbol{b_l}] = 0\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;Var%5b%5cboldsymbol%7bb_l%7d%5d%20%3d%200" title="Var[\boldsymbol{b_l}] = 0" />
  
</noscript>. Putting it all together we obtain equation 
<span class="jsonly">
                
      \((\ref{eq:fwd_var})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_var%7d%29" title="(\ref{eq:fwd_var})" />
  
</noscript> back:</p>


<span class="jsonly">
   
      $$d_l \times Var[y_l] = d_l \times n_l \times Var[w_lx_l] \Longleftrightarrow Var[y_l] = n_l Var[w_lx_l]$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?d_l%20%5ctimes%20Var%5by_l%5d%20%3d%20d_l%20%5ctimes%20n_l%20%5ctimes%20Var%5bw_lx_l%5d%20%5cLongleftrightarrow%20Var%5by_l%5d%20%3d%20n_l%20Var%5bw_lx_l%5d" title="d_l \times Var[y_l] = d_l \times n_l \times Var[w_lx_l] \Longleftrightarrow Var[y_l] = n_l Var[w_lx_l]" />
      </div>
  
</noscript>

<p>Pfiu. That was a mouthful. Let&rsquo;s go on!</p>

<p>We now make one more assumption: that the random variable 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> has zero mean. After all, setting the distribution of 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> is what we&rsquo;re trying to do here, so we can choose one that has zero mean. We can now further simplify equation 
<span class="jsonly">
                
      \((\ref{eq:fwd_var})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_var%7d%29" title="(\ref{eq:fwd_var})" />
  
</noscript> as follow (we also use once more the fact that 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> and 
<span class="jsonly">
                
      \(x_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_l" title="x_l" />
  
</noscript> are independent):</p>


<span class="jsonly">
   
      $$\begin{align} Var[y_l] &amp;= n_l Var[w_lx_l] \nonumber \\ \nonumber \\ &amp;= n_l \left( \underbrace{\mathbb{E}[w_l^2]}_{=Var[w_l]} \mathbb{E}[x_l^2] - \underbrace{\mathbb{E}[w_l]^2}_{=0}\mathbb{E}[x_l]^2 \right) \nonumber \\ \nonumber \\ &amp;= n_l Var[w_l] \mathbb{E}[x_l^2] \label{eq:fwd_var2} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20Var%5by_l%5d%20%26%3d%20n_l%20Var%5bw_lx_l%5d%20%5cnonumber%20%5c%5c%20%5cnonumber%20%5c%5c%20%26%3d%20n_l%20%5cleft%28%20%5cunderbrace%7b%5cmathbb%7bE%7d%5bw_l%5e2%5d%7d_%7b%3dVar%5bw_l%5d%7d%20%5cmathbb%7bE%7d%5bx_l%5e2%5d%20-%20%5cunderbrace%7b%5cmathbb%7bE%7d%5bw_l%5d%5e2%7d_%7b%3d0%7d%5cmathbb%7bE%7d%5bx_l%5d%5e2%20%5cright%29%20%5cnonumber%20%5c%5c%20%5cnonumber%20%5c%5c%20%26%3d%20n_l%20Var%5bw_l%5d%20%5cmathbb%7bE%7d%5bx_l%5e2%5d%20%5clabel%7beq%3afwd_var2%7d%20%5cend%7balign%7d" title="\begin{align} Var[y_l] &amp;= n_l Var[w_lx_l] \nonumber \\ \nonumber \\ &amp;= n_l \left( \underbrace{\mathbb{E}[w_l^2]}_{=Var[w_l]} \mathbb{E}[x_l^2] - \underbrace{\mathbb{E}[w_l]^2}_{=0}\mathbb{E}[x_l]^2 \right) \nonumber \\ \nonumber \\ &amp;= n_l Var[w_l] \mathbb{E}[x_l^2] \label{eq:fwd_var2} \end{align}" />
      </div>
  
</noscript>

<p>In those formulas, 
<span class="jsonly">
                
      \(\mathbb{E}[x_l^2]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cmathbb%7bE%7d%5bx_l%5e2%5d" title="\mathbb{E}[x_l^2]" />
  
</noscript> refers to the expectation of the square of 
<span class="jsonly">
                
      \(x_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_l" title="x_l" />
  
</noscript>.</p>

<p>Maybe you&rsquo;re asking yourself why I wrote that 
<span class="jsonly">
                
      \(\mathbb{E}[w_l^2] = Var[w_l]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cmathbb%7bE%7d%5bw_l%5e2%5d%20%3d%20Var%5bw_l%5d" title="\mathbb{E}[w_l^2] = Var[w_l]" />
  
</noscript> (which is true) but not 
<span class="jsonly">
                
      \(\mathbb{E}[x_l^2] = Var[x_l]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cmathbb%7bE%7d%5bx_l%5e2%5d%20%3d%20Var%5bx_l%5d" title="\mathbb{E}[x_l^2] = Var[x_l]" />
  
</noscript> (which is false). It&rsquo;s not (only) because it allowed me to get the same result as the paper. It&rsquo;s because 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> has zero mean, whereas 
<span class="jsonly">
                
      \(x_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_l" title="x_l" />
  
</noscript> does not. Thus, using the expression of the variance as a function of the expectation 
<span class="jsonly">
                
      \(Var[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;Var%5bX%5d%20%3d%20%5cmathbb%7bE%7d%5bX%5e2%5d%20-%20%5cmathbb%7bE%7d%5bX%5d%5e2" title="Var[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2" />
  
</noscript>, the square of the expectation of 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> is null but not the square of the expectation of 
<span class="jsonly">
                
      \(x_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_l" title="x_l" />
  
</noscript> .</p>

<p>Why doesn&rsquo;t 
<span class="jsonly">
                
      \(x_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_l" title="x_l" />
  
</noscript> have zero mean? That&rsquo;s because it is the result of the ReLU of the previous layer (
<span class="jsonly">
                
      \(x_l = max(0, y_{l-1})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_l%20%3d%20max%280%2c%20y_%7bl-1%7d%29" title="x_l = max(0, y_{l-1})" />
  
</noscript>) and thus it does not have zero mean. That is also one point on which Xavier and Kaiming differ: Xavier doesn&rsquo;t take into account the activation function. As a result, from here on <strong>we&rsquo;ll continue with the assumption that 
<span class="jsonly">
                
      \(x_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_l" title="x_l" />
  
</noscript> <em>is</em> the result of a ReLU, as in the Kaiming paper</strong>, because it&rsquo;s the more complex one. When we&rsquo;re finished, we&rsquo;ll look back to see what would have been different if we didn&rsquo;t make this assumption, like Xavier, and we&rsquo;ll see there&rsquo;s only a small thing that needs to be modified. As a reminder, here&rsquo;s the definition of the ReLU activation function:</p>


<span class="jsonly">
   
      $$ReLU(x) = \begin{cases} x \text{ if } x \geq 0 \\ 0 \text{ if } x &lt; 0 \end{cases} = \max(x, 0)$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?ReLU%28x%29%20%3d%20%5cbegin%7bcases%7d%20x%20%5ctext%7b%20if%20%7d%20x%20%5cgeq%200%20%5c%5c%200%20%5ctext%7b%20if%20%7d%20x%20%3c%200%20%5cend%7bcases%7d%20%3d%20%5cmax%28x%2c%200%29" title="ReLU(x) = \begin{cases} x \text{ if } x \geq 0 \\ 0 \text{ if } x &lt; 0 \end{cases} = \max(x, 0)" />
      </div>
  
</noscript>

<p>Now let&rsquo;s see what to do with 
<span class="jsonly">
                
      \(\mathbb{E}[x_{l}^2] = \mathbb{E}[max(0, y_{l-1})^2]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cmathbb%7bE%7d%5bx_%7bl%7d%5e2%5d%20%3d%20%5cmathbb%7bE%7d%5bmax%280%2c%20y_%7bl-1%7d%29%5e2%5d" title="\mathbb{E}[x_{l}^2] = \mathbb{E}[max(0, y_{l-1})^2]" />
  
</noscript>. We assume that 
<span class="jsonly">
                
      \(w_{l-1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_%7bl-1%7d" title="w_{l-1}" />
  
</noscript> has a symmetric distribution around 0 and that 
<span class="jsonly">
                
      \(b_{l-1} = 0\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;b_%7bl-1%7d%20%3d%200" title="b_{l-1} = 0" />
  
</noscript>. Then 
<span class="jsonly">
                
      \(y_{l-1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;y_%7bl-1%7d" title="y_{l-1}" />
  
</noscript> will also have a symmetric distribution around 0, and will have zero mean. Indeed, taking the expectation of equation 
<span class="jsonly">
                
      \((\ref{eq:fwd})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd%7d%29" title="(\ref{eq:fwd})" />
  
</noscript> gives us (because 
<span class="jsonly">
                
      \(w_{l-1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_%7bl-1%7d" title="w_{l-1}" />
  
</noscript> has zero mean and is independent from 
<span class="jsonly">
                
      \(x_{l-1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_%7bl-1%7d" title="x_{l-1}" />
  
</noscript>):

<span class="jsonly">
   
      $$\mathbb{E}(y_{l-1}) = \mathbb{E}(w_{l-1} x_{l-1}) = \mathbb{E}(w_{l-1})\mathbb{E}(x_{l-1}) = 0$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cmathbb%7bE%7d%28y_%7bl-1%7d%29%20%3d%20%5cmathbb%7bE%7d%28w_%7bl-1%7d%20x_%7bl-1%7d%29%20%3d%20%5cmathbb%7bE%7d%28w_%7bl-1%7d%29%5cmathbb%7bE%7d%28x_%7bl-1%7d%29%20%3d%200" title="\mathbb{E}(y_{l-1}) = \mathbb{E}(w_{l-1} x_{l-1}) = \mathbb{E}(w_{l-1})\mathbb{E}(x_{l-1}) = 0" />
      </div>
  
</noscript></p>

<p>And because 
<span class="jsonly">
                
      \(w_{l-1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_%7bl-1%7d" title="w_{l-1}" />
  
</noscript> has zero mean and is distributed symmetrically around 0:</p>


<span class="jsonly">
   
      $$\begin{align*} \mathbb{P}(y_{l-1} &gt; 0) &amp;= \mathbb{P} ( w_{l-1}x_{l-1} &gt; 0) \\ &amp;= \mathbb{P}\left( (w_{l-1} &gt; 0 \text{ and } x_{l-1} &gt; 0 ) \text{ or } (w_{l-1} &lt; 0 \text{ and } x_{l-1} &lt; 0)\right) \\ &amp;= \mathbb{P}(w_{l-1} &gt;0 ) \mathbb{P}(x_{l-1} &gt; 0) &#43;  \mathbb{P}(w_{l-1} &lt; 0 ) \mathbb{P}(x_{l-1} &lt; 0) \\  &amp;= \frac{1}{2} \mathbb{P} ( x_{l-1} &gt; 0) &#43; \frac{1}{2} \mathbb{P} ( x_{l-1} &lt; 0) \\  &amp;= \frac{1}{2} \end{align*}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%2a%7d%20%5cmathbb%7bP%7d%28y_%7bl-1%7d%20%3e%200%29%20%26%3d%20%5cmathbb%7bP%7d%20%28%20w_%7bl-1%7dx_%7bl-1%7d%20%3e%200%29%20%5c%5c%20%26%3d%20%5cmathbb%7bP%7d%5cleft%28%20%28w_%7bl-1%7d%20%3e%200%20%5ctext%7b%20and%20%7d%20x_%7bl-1%7d%20%3e%200%20%29%20%5ctext%7b%20or%20%7d%20%28w_%7bl-1%7d%20%3c%200%20%5ctext%7b%20and%20%7d%20x_%7bl-1%7d%20%3c%200%29%5cright%29%20%5c%5c%20%26%3d%20%5cmathbb%7bP%7d%28w_%7bl-1%7d%20%3e0%20%29%20%5cmathbb%7bP%7d%28x_%7bl-1%7d%20%3e%200%29%20%2b%20%20%5cmathbb%7bP%7d%28w_%7bl-1%7d%20%3c%200%20%29%20%5cmathbb%7bP%7d%28x_%7bl-1%7d%20%3c%200%29%20%5c%5c%20%20%26%3d%20%5cfrac%7b1%7d%7b2%7d%20%5cmathbb%7bP%7d%20%28%20x_%7bl-1%7d%20%3e%200%29%20%2b%20%5cfrac%7b1%7d%7b2%7d%20%5cmathbb%7bP%7d%20%28%20x_%7bl-1%7d%20%3c%200%29%20%5c%5c%20%20%26%3d%20%5cfrac%7b1%7d%7b2%7d%20%5cend%7balign%2a%7d" title="\begin{align*} \mathbb{P}(y_{l-1} &gt; 0) &amp;= \mathbb{P} ( w_{l-1}x_{l-1} &gt; 0) \\ &amp;= \mathbb{P}\left( (w_{l-1} &gt; 0 \text{ and } x_{l-1} &gt; 0 ) \text{ or } (w_{l-1} &lt; 0 \text{ and } x_{l-1} &lt; 0)\right) \\ &amp;= \mathbb{P}(w_{l-1} &gt;0 ) \mathbb{P}(x_{l-1} &gt; 0) &#43;  \mathbb{P}(w_{l-1} &lt; 0 ) \mathbb{P}(x_{l-1} &lt; 0) \\  &amp;= \frac{1}{2} \mathbb{P} ( x_{l-1} &gt; 0) &#43; \frac{1}{2} \mathbb{P} ( x_{l-1} &lt; 0) \\  &amp;= \frac{1}{2} \end{align*}" />
      </div>
  
</noscript>

<p>Now that we have established that 
<span class="jsonly">
                
      \(y_{l-1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;y_%7bl-1%7d" title="y_{l-1}" />
  
</noscript> is indeed centered on 0 and symmetric, we can compute the expectation of 
<span class="jsonly">
                
      \(x_l^2\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_l%5e2" title="x_l^2" />
  
</noscript>:</p>


<span class="jsonly">
   
      $$\begin{align*} \mathbb{E}[x_l^2] &amp;= \mathbb{E}\left[max(0, y_{l-1})^2 \right] \\ &amp;= \frac{1}{2} \mathbb{E}\left[y_{l-1}^2\right] \\ &amp;= \frac{1}{2} Var[y_{l-1}] \end{align*}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%2a%7d%20%5cmathbb%7bE%7d%5bx_l%5e2%5d%20%26%3d%20%5cmathbb%7bE%7d%5cleft%5bmax%280%2c%20y_%7bl-1%7d%29%5e2%20%5cright%5d%20%5c%5c%20%26%3d%20%5cfrac%7b1%7d%7b2%7d%20%5cmathbb%7bE%7d%5cleft%5by_%7bl-1%7d%5e2%5cright%5d%20%5c%5c%20%26%3d%20%5cfrac%7b1%7d%7b2%7d%20Var%5by_%7bl-1%7d%5d%20%5cend%7balign%2a%7d" title="\begin{align*} \mathbb{E}[x_l^2] &amp;= \mathbb{E}\left[max(0, y_{l-1})^2 \right] \\ &amp;= \frac{1}{2} \mathbb{E}\left[y_{l-1}^2\right] \\ &amp;= \frac{1}{2} Var[y_{l-1}] \end{align*}" />
      </div>
  
</noscript>

<p>Plugging this back into equation 
<span class="jsonly">
                
      \((\ref{eq:fwd_var2})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_var2%7d%29" title="(\ref{eq:fwd_var2})" />
  
</noscript>, we get:</p>


<span class="jsonly">
   
      $$Var[y_l] = \frac{1}{2} n_l Var[w_l] Var[y_{l-1}]$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?Var%5by_l%5d%20%3d%20%5cfrac%7b1%7d%7b2%7d%20n_l%20Var%5bw_l%5d%20Var%5by_%7bl-1%7d%5d" title="Var[y_l] = \frac{1}{2} n_l Var[w_l] Var[y_{l-1}]" />
      </div>
  
</noscript>

<p>Bingo! We now have a recurrence equation between the activations at layer 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript> and the activations at layer 
<span class="jsonly">
                
      \(l-1\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l-1" title="l-1" />
  
</noscript>. Starting from the last layer 
<span class="jsonly">
                
      \(L\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;L" title="L" />
  
</noscript>, we can thus form the following product:</p>


<span class="jsonly">
   
      $$\begin{align} Var[y_L] = Var[y_1] \left( \prod_{l=2}^{L} \frac{1}{2} n_l Var[w_l] \right) \label{eq:fwd_prod} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20Var%5by_L%5d%20%3d%20Var%5by_1%5d%20%5cleft%28%20%5cprod_%7bl%3d2%7d%5e%7bL%7d%20%5cfrac%7b1%7d%7b2%7d%20n_l%20Var%5bw_l%5d%20%5cright%29%20%5clabel%7beq%3afwd_prod%7d%20%5cend%7balign%7d" title="\begin{align} Var[y_L] = Var[y_1] \left( \prod_{l=2}^{L} \frac{1}{2} n_l Var[w_l] \right) \label{eq:fwd_prod} \end{align}" />
      </div>
  
</noscript>

<p>This formula is the one that lets us see what could go wrong without a proper initialization, and thus how to design the right one. The product is key. Indeed, if we have a lot of layers (so if 
<span class="jsonly">
                
      \(L\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;L" title="L" />
  
</noscript> is large), we see that the variance of the last layer 
<span class="jsonly">
                
      \(y_L\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;y_L" title="y_L" />
  
</noscript> could be very small (if 
<span class="jsonly">
                
      \(\frac{1}{2} n_l Var[w_l]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cfrac%7b1%7d%7b2%7d%20n_l%20Var%5bw_l%5d" title="\frac{1}{2} n_l Var[w_l]" />
  
</noscript> is below 1) or very large ( if 
<span class="jsonly">
                
      \(\frac{1}{2} n_l Var[w_l]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cfrac%7b1%7d%7b2%7d%20n_l%20Var%5bw_l%5d" title="\frac{1}{2} n_l Var[w_l]" />
  
</noscript> is above 1). The proper value for what&rsquo;s inside that product should thus be 1, and that is exactly the sufficient condition the Kaiming paper (and initialization) takes:</p>


<span class="jsonly">
   
      $$\begin{align} \forall l \,, \; \frac{1}{2} n_l Var[w_l] = 1 \label{eq:fwd_K} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20%5cforall%20l%20%5c%2c%2c%20%5c%3b%20%5cfrac%7b1%7d%7b2%7d%20n_l%20Var%5bw_l%5d%20%3d%201%20%5clabel%7beq%3afwd_K%7d%20%5cend%7balign%7d" title="\begin{align} \forall l \,, \; \frac{1}{2} n_l Var[w_l] = 1 \label{eq:fwd_K} \end{align}" />
      </div>
  
</noscript>

<p>The Kaiming paper accordingly suggests to initialize the weights of layer 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript> with a zero-mean Gaussian distribution with a standard deviation of 
<span class="jsonly">
                
      \(\sqrt{\frac{2}{n_l}}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5csqrt%7b%5cfrac%7b2%7d%7bn_l%7d%7d" title="\sqrt{\frac{2}{n_l}}" />
  
</noscript>, and null biases.</p>

<p>The standard deviation of the first layer isn&rsquo;t that important. Indeed, looking at equation 
<span class="jsonly">
                
      \((\ref{eq:fwd_prod})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_prod%7d%29" title="(\ref{eq:fwd_prod})" />
  
</noscript> we can see that the variance of the first layer won&rsquo;t contribute to an exponentially small or large variance at the last layer. For simplicity, we also use the same equation 
<span class="jsonly">
                
      \((\ref{eq:fwd_K})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_K%7d%29" title="(\ref{eq:fwd_K})" />
  
</noscript> to define the standard deviation of the first layer, even though to be 100% rigorous that would lead to a variance of <sup>1</sup>&frasl;<sub>2</sub>, because there is not ReLU applied to the input.</p>

<p>That&rsquo;s it for the Kaiming initialization in the forward case! Xavier initialization is a tiny bit different: as the activation function was assumed to be linear by Xavier (or at least approximated to be linear with a derivative of 1 around 0), it&rsquo;s not taken into account, and thus the <sup>1</sup>&frasl;<sub>2</sub> that comes from ReLU isn&rsquo;t there. The Xavier initialization formula in the forward case is hence:</p>


<span class="jsonly">
   
      $$\begin{align} \forall l \,, \; n_l Var[w_l] = 1 \label{eq:fwd_X} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20%5cforall%20l%20%5c%2c%2c%20%5c%3b%20n_l%20Var%5bw_l%5d%20%3d%201%20%5clabel%7beq%3afwd_X%7d%20%5cend%7balign%7d" title="\begin{align} \forall l \,, \; n_l Var[w_l] = 1 \label{eq:fwd_X} \end{align}" />
      </div>
  
</noscript>

<p>Let&rsquo;s tackle back-prop now!</p>

<h3 id="backward-propagation">Backward-propagation</h3>

<p>The backward-propagation story is very similar to the forward-propagation one. The only difference is that we&rsquo;re dealing with gradients, and that the flow is now from the last layers to the first ones. Accordingly, we&rsquo;ll have a very similar but still slightly different result.</p>

<p>Instead of the forward-propagation equation 
<span class="jsonly">
                
      \((\ref{eq:fwd})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd%7d%29" title="(\ref{eq:fwd})" />
  
</noscript> we had, the formula of the gradient of layer 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript> is given by:</p>


<span class="jsonly">
   
      $$\begin{align}\Delta \boldsymbol{x_l} = \hat{W}_l \Delta \boldsymbol{y_l} \label{eq:bwd} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%5cDelta%20%5cboldsymbol%7bx_l%7d%20%3d%20%5chat%7bW%7d_l%20%5cDelta%20%5cboldsymbol%7by_l%7d%20%5clabel%7beq%3abwd%7d%20%5cend%7balign%7d" title="\begin{align}\Delta \boldsymbol{x_l} = \hat{W}_l \Delta \boldsymbol{y_l} \label{eq:bwd} \end{align}" />
      </div>
  
</noscript>

<p>Notice the bold symbols again. Sneaky sneaky. The terms of equation 
<span class="jsonly">
                
      \((\ref{eq:bwd})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3abwd%7d%29" title="(\ref{eq:bwd})" />
  
</noscript> are defined as follow:</p>

<ul>
<li><p>
<span class="jsonly">
                
      \(\Delta \boldsymbol{y_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20%5cboldsymbol%7by_l%7d" title="\Delta \boldsymbol{y_l}" />
  
</noscript> is the 
<span class="jsonly">
                
      \(\hat{n}_l\text{-by-}1\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bn%7d_l%5ctext%7b-by-%7d1" title="\hat{n}_l\text{-by-}1" />
  
</noscript> gradient of the activation vector (pre-ReLU). 
<span class="jsonly">
                
      \(\hat{n}_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bn%7d_l" title="\hat{n}_l" />
  
</noscript> has a slightly different definition than 
<span class="jsonly">
                
      \(n_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;n_l" title="n_l" />
  
</noscript>, which I won&rsquo;t get into. It&rsquo;s not that important in the long run, just note that 
<span class="jsonly">
                
      \(\hat{n}_l \not = n_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bn%7d_l%20%5cnot%20%3d%20n_l" title="\hat{n}_l \not = n_l" />
  
</noscript> because we now move through the network in the other direction.</p></li>

<li><p>
<span class="jsonly">
                
      \(\hat{W}_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bW%7d_l" title="\hat{W}_l" />
  
</noscript> is a 
<span class="jsonly">
                
      \(c_l \text{-by-}\hat{n}_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;c_l%20%5ctext%7b-by-%7d%5chat%7bn%7d_l" title="c_l \text{-by-}\hat{n}_l" />
  
</noscript> weight matrix, which is just a rearrangement of 
<span class="jsonly">
                
      \(W_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;W_l" title="W_l" />
  
</noscript> to compute backward-propagation.</p></li>

<li><p>
<span class="jsonly">
                
      \(\Delta \boldsymbol{x_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20%5cboldsymbol%7bx_l%7d" title="\Delta \boldsymbol{x_l}" />
  
</noscript> is a 
<span class="jsonly">
                
      \(c_l\text{-by-}1\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;c_l%5ctext%7b-by-%7d1" title="c_l\text{-by-}1" />
  
</noscript> vector representing the gradient of a pixel of layer 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript>.</p></li>
</ul>

<p>We also make very similar assumptions to the forward-propagation case: 
<span class="jsonly">
                
      \(\hat{W}_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bW%7d_l" title="\hat{W}_l" />
  
</noscript> and 
<span class="jsonly">
                
      \(\Delta \boldsymbol{y_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20%5cboldsymbol%7by_l%7d" title="\Delta \boldsymbol{y_l}" />
  
</noscript> contain random variables that have the same distribution, respectively. Accordingly, we note 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> and 
<span class="jsonly">
                
      \(\Delta y_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20y_l" title="\Delta y_l" />
  
</noscript> random variables that follow the same distribution as the ones in 
<span class="jsonly">
                
      \(\hat{W}_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bW%7d_l" title="\hat{W}_l" />
  
</noscript> and 
<span class="jsonly">
                
      \(\Delta \boldsymbol{y_l}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20%5cboldsymbol%7by_l%7d" title="\Delta \boldsymbol{y_l}" />
  
</noscript>, respectively. A hat wasn&rsquo;t put on 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> because 
<span class="jsonly">
                
      \(W_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;W_l" title="W_l" />
  
</noscript> and 
<span class="jsonly">
                
      \(\hat{W}_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bW%7d_l" title="\hat{W}_l" />
  
</noscript> are the same matrices, only one is rearranged from the other (so their random variables are the same). We also assume that 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> and 
<span class="jsonly">
                
      \(\Delta y_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20y_l" title="\Delta y_l" />
  
</noscript> are independent of each other. Finally, 
<span class="jsonly">
                
      \(w_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;w_l" title="w_l" />
  
</noscript> has a symmetric distribution around 0 so 
<span class="jsonly">
                
      \(\Delta x_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20x_l" title="\Delta x_l" />
  
</noscript> has zero mean for all 
<span class="jsonly">
                
      \(l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;l" title="l" />
  
</noscript>.</p>

<p>Moreover, backward-propagation involves the following equation 
<span class="jsonly">
                
      \(\Delta y_l = f&#39;(y_l) \Delta x_{l&#43;1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20y_l%20%3d%20f%27%28y_l%29%20%5cDelta%20x_%7bl%2b1%7d" title="\Delta y_l = f&#39;(y_l) \Delta x_{l&#43;1}" />
  
</noscript>, where 
<span class="jsonly">
                
      \(f\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;f" title="f" />
  
</noscript> is the activation function. Convince yourself of this by writing the partial derivation of the cost function. In the case of ReLU, which we use as the active function here, here&rsquo;s the derivative:</p>


<span class="jsonly">
   
      $$f&#39;(x) = ReLU&#39;(x) = \begin{cases} 1 \quad \text{ if } x \geq 0 \\ 0 \quad \text{ if } x &lt; 0 \end{cases}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?f%27%28x%29%20%3d%20ReLU%27%28x%29%20%3d%20%5cbegin%7bcases%7d%201%20%5cquad%20%5ctext%7b%20if%20%7d%20x%20%5cgeq%200%20%5c%5c%200%20%5cquad%20%5ctext%7b%20if%20%7d%20x%20%3c%200%20%5cend%7bcases%7d" title="f&#39;(x) = ReLU&#39;(x) = \begin{cases} 1 \quad \text{ if } x \geq 0 \\ 0 \quad \text{ if } x &lt; 0 \end{cases}" />
      </div>
  
</noscript>

<p>One last assumption: 
<span class="jsonly">
                
      \(f&#39;(y_l)\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;f%27%28y_l%29" title="f&#39;(y_l)" />
  
</noscript> and 
<span class="jsonly">
                
      \(\Delta x_{l&#43;1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20x_%7bl%2b1%7d" title="\Delta x_{l&#43;1}" />
  
</noscript> are independent from each other. Thus, taking into account that ReLU &lsquo;halves&rsquo; its input, at least if the input is symmetric around 0, we have:</p>


<span class="jsonly">
   
      $$\begin{align*} \mathbb{E}[\Delta y_l] &amp;= \mathbb{E}\left( \, f&#39;(y_l) \Delta x_{l&#43;1}\right) \\ &amp;=\frac{1}{2} \mathbb{E}[\Delta x_{l&#43;1}] = 0 \end{align*}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%2a%7d%20%5cmathbb%7bE%7d%5b%5cDelta%20y_l%5d%20%26%3d%20%5cmathbb%7bE%7d%5cleft%28%20%5c%2c%20f%27%28y_l%29%20%5cDelta%20x_%7bl%2b1%7d%5cright%29%20%5c%5c%20%26%3d%5cfrac%7b1%7d%7b2%7d%20%5cmathbb%7bE%7d%5b%5cDelta%20x_%7bl%2b1%7d%5d%20%3d%200%20%5cend%7balign%2a%7d" title="\begin{align*} \mathbb{E}[\Delta y_l] &amp;= \mathbb{E}\left( \, f&#39;(y_l) \Delta x_{l&#43;1}\right) \\ &amp;=\frac{1}{2} \mathbb{E}[\Delta x_{l&#43;1}] = 0 \end{align*}" />
      </div>
  
</noscript>

<p>and because 
<span class="jsonly">
                
      \(\Delta y_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20y_l" title="\Delta y_l" />
  
</noscript> has a zero mean, and that 
<span class="jsonly">
                
      \(f&#39;(y_l)^2 = f&#39;(y_l)\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;f%27%28y_l%29%5e2%20%3d%20f%27%28y_l%29" title="f&#39;(y_l)^2 = f&#39;(y_l)" />
  
</noscript>, by taking the expectation of the square of the equation 
<span class="jsonly">
                
      \(\Delta y_l = f&#39;(y_l) \Delta x_{l&#43;1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20y_l%20%3d%20f%27%28y_l%29%20%5cDelta%20x_%7bl%2b1%7d" title="\Delta y_l = f&#39;(y_l) \Delta x_{l&#43;1}" />
  
</noscript>, we get:</p>


<span class="jsonly">
   
      $$\begin{align*} \mathbb{E}\left[(\Delta y_l)^2\right] = Var[\Delta y_l] &amp;= \mathbb{E}\left( \, f&#39;(y_l)^2 \Delta x_{l&#43;1} ^2 \right) \\ &amp;= \frac{1}{2} \mathbb{E}(\Delta x_{l&#43;1}^2) \\&amp;= \frac{1}{2} Var[\Delta x_{l&#43;1}] \end{align*}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%2a%7d%20%5cmathbb%7bE%7d%5cleft%5b%28%5cDelta%20y_l%29%5e2%5cright%5d%20%3d%20Var%5b%5cDelta%20y_l%5d%20%26%3d%20%5cmathbb%7bE%7d%5cleft%28%20%5c%2c%20f%27%28y_l%29%5e2%20%5cDelta%20x_%7bl%2b1%7d%20%5e2%20%5cright%29%20%5c%5c%20%26%3d%20%5cfrac%7b1%7d%7b2%7d%20%5cmathbb%7bE%7d%28%5cDelta%20x_%7bl%2b1%7d%5e2%29%20%5c%5c%26%3d%20%5cfrac%7b1%7d%7b2%7d%20Var%5b%5cDelta%20x_%7bl%2b1%7d%5d%20%5cend%7balign%2a%7d" title="\begin{align*} \mathbb{E}\left[(\Delta y_l)^2\right] = Var[\Delta y_l] &amp;= \mathbb{E}\left( \, f&#39;(y_l)^2 \Delta x_{l&#43;1} ^2 \right) \\ &amp;= \frac{1}{2} \mathbb{E}(\Delta x_{l&#43;1}^2) \\&amp;= \frac{1}{2} Var[\Delta x_{l&#43;1}] \end{align*}" />
      </div>
  
</noscript>

<p>To remind you, we do have 
<span class="jsonly">
                
      \(\mathbb{E}(\Delta x_{l&#43;1} ^2) = Var[\Delta x_{l&#43;1}]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cmathbb%7bE%7d%28%5cDelta%20x_%7bl%2b1%7d%20%5e2%29%20%3d%20Var%5b%5cDelta%20x_%7bl%2b1%7d%5d" title="\mathbb{E}(\Delta x_{l&#43;1} ^2) = Var[\Delta x_{l&#43;1}]" />
  
</noscript> because 
<span class="jsonly">
                
      \(\Delta x_{l&#43;1}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20x_%7bl%2b1%7d" title="\Delta x_{l&#43;1}" />
  
</noscript> has zero mean as we just demonstrated.</p>

<p>We now have everything we need to compute the variance of equation 
<span class="jsonly">
                
      \((\ref{eq:bwd})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3abwd%7d%29" title="(\ref{eq:bwd})" />
  
</noscript>, with the exact same reasoning we had with the forward case (remember the big matrices?):</p>


<span class="jsonly">
   
      $$\begin{align} Var[\Delta x_l] &amp;= Var[\hat{W}_l \Delta \boldsymbol{y_l}] \nonumber \\ &amp;= \hat{n}_l Var[w_l] Var[\Delta y_l] \nonumber \\ &amp;= \frac{1}{2} \hat{n}_l Var[w_l]Var[\Delta x_{l&#43;1}] \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20Var%5b%5cDelta%20x_l%5d%20%26%3d%20Var%5b%5chat%7bW%7d_l%20%5cDelta%20%5cboldsymbol%7by_l%7d%5d%20%5cnonumber%20%5c%5c%20%26%3d%20%5chat%7bn%7d_l%20Var%5bw_l%5d%20Var%5b%5cDelta%20y_l%5d%20%5cnonumber%20%5c%5c%20%26%3d%20%5cfrac%7b1%7d%7b2%7d%20%5chat%7bn%7d_l%20Var%5bw_l%5dVar%5b%5cDelta%20x_%7bl%2b1%7d%5d%20%5cend%7balign%7d" title="\begin{align} Var[\Delta x_l] &amp;= Var[\hat{W}_l \Delta \boldsymbol{y_l}] \nonumber \\ &amp;= \hat{n}_l Var[w_l] Var[\Delta y_l] \nonumber \\ &amp;= \frac{1}{2} \hat{n}_l Var[w_l]Var[\Delta x_{l&#43;1}] \end{align}" />
      </div>
  
</noscript>

<p>As before, we have a recurrence equation. This time it&rsquo;s on 
<span class="jsonly">
                
      \(\Delta x_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cDelta%20x_l" title="\Delta x_l" />
  
</noscript> rather than 
<span class="jsonly">
                
      \(y_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;y_l" title="y_l" />
  
</noscript>, but we can turn it into a product over all the 
<span class="jsonly">
                
      \(L\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;L" title="L" />
  
</noscript> layers all the same:</p>


<span class="jsonly">
   
      $$ \begin{align} Var[\Delta x_2] = Var[\Delta x_{L&#43;1}] \left( \prod_{l=2}^{L} \frac{1}{2} \hat{n}_l Var[w_l] \right) \label{eq:bwd_prod} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%20%5cbegin%7balign%7d%20Var%5b%5cDelta%20x_2%5d%20%3d%20Var%5b%5cDelta%20x_%7bL%2b1%7d%5d%20%5cleft%28%20%5cprod_%7bl%3d2%7d%5e%7bL%7d%20%5cfrac%7b1%7d%7b2%7d%20%5chat%7bn%7d_l%20Var%5bw_l%5d%20%5cright%29%20%5clabel%7beq%3abwd_prod%7d%20%5cend%7balign%7d" title=" \begin{align} Var[\Delta x_2] = Var[\Delta x_{L&#43;1}] \left( \prod_{l=2}^{L} \frac{1}{2} \hat{n}_l Var[w_l] \right) \label{eq:bwd_prod} \end{align}" />
      </div>
  
</noscript>

<p>Remember 
<span class="jsonly">
                
      \(x_1\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;x_1" title="x_1" />
  
</noscript> is the input of the network, that&rsquo;s why the above equation &ldquo;begins&rdquo; at 2.</p>

<p>Again, as before, this product is key to understanding why the right initialization is so important: if not set carefully, the gradient can explode or vanish, depending on wether 
<span class="jsonly">
                
      \(\frac{1}{2} \hat{n}_l Var[w_l]\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5cfrac%7b1%7d%7b2%7d%20%5chat%7bn%7d_l%20Var%5bw_l%5d" title="\frac{1}{2} \hat{n}_l Var[w_l]" />
  
</noscript> is over or below 1 (strictly). Setting that number to 1 is important:</p>


<span class="jsonly">
   
      $$\begin{align} \forall l \,, \; \frac{1}{2}\hat{n}_l Var[w_l] = 1 \label{eq:bwd_K} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20%5cforall%20l%20%5c%2c%2c%20%5c%3b%20%5cfrac%7b1%7d%7b2%7d%5chat%7bn%7d_l%20Var%5bw_l%5d%20%3d%201%20%5clabel%7beq%3abwd_K%7d%20%5cend%7balign%7d" title="\begin{align} \forall l \,, \; \frac{1}{2}\hat{n}_l Var[w_l] = 1 \label{eq:bwd_K} \end{align}" />
      </div>
  
</noscript>

<p>Thus we can choose a distribution with a standard deviation of 
<span class="jsonly">
                
      \(\sqrt{\frac{2}{\hat{n}_l}}\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5csqrt%7b%5cfrac%7b2%7d%7b%5chat%7bn%7d_l%7d%7d" title="\sqrt{\frac{2}{\hat{n}_l}}" />
  
</noscript>. Lastly, same as before, the first layer wouldn&rsquo;t be at the same standard deviation because no activation function is used, but the small <sup>1</sup>&frasl;<sub>2</sub> factor doesn&rsquo;t matter because it doesn&rsquo;t get compounded over 
<span class="jsonly">
                
      \(L\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;L" title="L" />
  
</noscript> layers.</p>

<p>The difference between this Kaiming initialization and the Xavier one is again the <sup>1</sup>&frasl;<sub>2</sub> that comes from the ReLU activation function. Careful, that <sup>1</sup>&frasl;<sub>2</sub> isn&rsquo;t the one I&rsquo;ve just talked about in the previous paragraph, and it&rsquo;s not even coming the same way from the ReLU. The Xavier initialization in the backward-propagation case comes from the equation:</p>


<span class="jsonly">
   
      $$\begin{align} \forall l \,, \; \hat{n}_l Var[w_l] = 1 \label{eq:bwd_X} \end{align}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5cbegin%7balign%7d%20%5cforall%20l%20%5c%2c%2c%20%5c%3b%20%5chat%7bn%7d_l%20Var%5bw_l%5d%20%3d%201%20%5clabel%7beq%3abwd_X%7d%20%5cend%7balign%7d" title="\begin{align} \forall l \,, \; \hat{n}_l Var[w_l] = 1 \label{eq:bwd_X} \end{align}" />
      </div>
  
</noscript>

<p>Yay! We figured out backward-propagation!</p>

<h3 id="bringing-forward-propagation-and-backward-propagation-together">Bringing forward-propagation and backward-propagation together</h3>

<p>So we have two equations very similar to each other that suggest a slightly different value for the initialization: 
<span class="jsonly">
                
      \((\ref{eq:fwd_K})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_K%7d%29" title="(\ref{eq:fwd_K})" />
  
</noscript> and 
<span class="jsonly">
                
      \((\ref{eq:bwd_K})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3abwd_K%7d%29" title="(\ref{eq:bwd_K})" />
  
</noscript> for Kaiming, and 
<span class="jsonly">
                
      \((\ref{eq:fwd_X})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_X%7d%29" title="(\ref{eq:fwd_X})" />
  
</noscript> and 
<span class="jsonly">
                
      \((\ref{eq:bwd_X})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3abwd_X%7d%29" title="(\ref{eq:bwd_X})" />
  
</noscript> for Xavier. One is the forward-propagation, the other the backward-propagation. Once again, the Xavier and the Kaiming papers take two different approaches. Xavier offers to just take the (harmonic) mean of the two (Xavier) initializations, which amounts to initializing the weights with a distribution of standard deviation of:

<span class="jsonly">
   
      $$\sigma = \sqrt{\frac{2}{n_l &#43; \hat{n}_l}}$$
  
</span>
<noscript>
   
      <div style="text-align:center;">
          <img src="https://latex.codecogs.com/gif.latex?%5csigma%20%3d%20%5csqrt%7b%5cfrac%7b2%7d%7bn_l%20%2b%20%5chat%7bn%7d_l%7d%7d" title="\sigma = \sqrt{\frac{2}{n_l &#43; \hat{n}_l}}" />
      </div>
  
</noscript></p>

<p>On the other hand, Kaiming argues that both 
<span class="jsonly">
                
      \((\ref{eq:fwd_K})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_K%7d%29" title="(\ref{eq:fwd_K})" />
  
</noscript> and 
<span class="jsonly">
                
      \((\ref{eq:bwd_K})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3abwd_K%7d%29" title="(\ref{eq:bwd_K})" />
  
</noscript> properly scale the forward and the backward signals. Indeed, if we compute the product of the quotient of 
<span class="jsonly">
                
      \(n_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;n_l" title="n_l" />
  
</noscript> and 
<span class="jsonly">
                
      \(\hat{n}_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bn%7d_l" title="\hat{n}_l" />
  
</noscript> over all 
<span class="jsonly">
                
      \(L\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;L" title="L" />
  
</noscript> layers we find a constant that only depends on the number of channels at the beginning of the network and at the end. I&rsquo;m not computing that number here because I didn&rsquo;t detailed what 
<span class="jsonly">
                
      \(n_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;n_l" title="n_l" />
  
</noscript> and 
<span class="jsonly">
                
      \(\hat{n}_l\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%5chat%7bn%7d_l" title="\hat{n}_l" />
  
</noscript> were in terms of the other parameters of the network. If you&rsquo;re interested, take a look at the end of the backward-propagation subsection in the 2.2 section of the Kaiming paper. Anyway, that number isn&rsquo;t at risk of making the weights or the gradient explode or vanish, so it&rsquo;s safe to just use one of 
<span class="jsonly">
                
      \((\ref{eq:fwd_K})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3afwd_K%7d%29" title="(\ref{eq:fwd_K})" />
  
</noscript> and 
<span class="jsonly">
                
      \((\ref{eq:bwd_K})\)
  
</span>
<noscript>
                
      <img style="display:inline;vertical-align:middle;" src="https://latex.codecogs.com/gif.latex?\inline&space;%28%5cref%7beq%3abwd_K%7d%29" title="(\ref{eq:bwd_K})" />
  
</noscript>. No need to compute the two cases after all! Aren&rsquo;t you glad I didn&rsquo;t tell you this in the introduction, and I made you read the whole thing?</p>

<h2 id="the-results">The results</h2>

<p>First, let&rsquo;s take a look at the contribution of Xavier initialization.</p>

<p>As a reminder, here&rsquo;s what activations and gradients looked like with the standard initialization:</p>

<p><img src="/initialization/activation_std.png#center" alt="activ std init" /></p>

<p align="center"><i> Activation values with standard initialization, <a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank"> source: Xavier paper </a> </i></p>

<p><img src="/initialization/back_prop_grad_std.png#center" alt="grad std init" /></p>

<p align="center"><i> Gradient values with standard initialization, <a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank"> source: Xavier paper </a> </i></p>

<p>And here&rsquo;s what they look like with Xavier initialization:</p>

<p><img src="/initialization/activation_norm.png#center" alt="activ Xavier init" /></p>

<p align="center"><i> Activation values with Xavier initialization, <a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank"> source: Xavier paper </a> </i></p>

<p><img src="/initialization/back_prop_grad_norm.png#center" alt="grad Xavier init" /></p>

<p align="center"><i> Gradient values with Xavier initialization, <a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank"> source: Xavier paper </a> </i></p>

<p>That&rsquo;s way better right?! Now the values don&rsquo;t seem to change at all! Well, to be fair, in the activation graph layer 5 seems a tiny bit below layer 1, but hey.</p>

<p>As we saw, Kaiming initialization is more accurate than Xavier initialization, especially if the activation function doesn&rsquo;t have a derivative of 1 at 0, like ReLU: in that case, the linear approximation of Xavier initialization is quite bad. And that badness will get compounded over all the layers so the deeper the network, the worse it&rsquo;ll get.</p>

<p>Indeed, the Kaiming paper compares the convergence of Xavier initialization and Kaiming initialization, with a 22-layer model and with a 30-layer model, with ReLU as the activation function in both cases.</p>

<p>Here&rsquo;s the graph for the 22-layer model:</p>

<p><img src="/initialization/converge_22layers.png#center" alt="22-layer" /></p>

<p align="center"><i> Error rate as a function of epochs with Xavier vs Kaiming initialization, 22-layer model <a href="https://arxiv.org/abs/1502.01852" target="_blank"> source: Kaiming paper </a> </i></p>

<p>In this case, Xavier initialization doesn&rsquo;t fare too badly. It converges a bit more slowly than Kaiming initialization, but the Kaiming paper notes that both kinds of initialization lead to the same accuracy.</p>

<p>Here&rsquo;s the graph for the 30-layer model:</p>

<p><img src="/initialization/converge_30layers.png#center" alt="30-layer" /></p>

<p align="center"><i> Error rate as a function of epochs with Xavier vs Kaiming initialization, 30-layer model <a href="https://arxiv.org/abs/1502.01852" target="_blank"> source: Kaiming paper </a> </i></p>

<p>In this case, Xavier initialization doesn&rsquo;t even converge! This graph in particular highlights the need for Kaiming initialization if ReLU is used (and it&rsquo;s kind of always used nowadays). This effect would be even worse for deep networks.</p>

<p>That&rsquo;s it! I hope you liked my synthesis of Xavier and Kaiming initialization , and that it has helped you to understand them both.</p>

<p>Feel free to reach out to me on Twitter or by email if you have any question or suggestion (or if you find a typo).</p>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/blog/decorators/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Finally understanding decorators in Python</span>
    </a>
    
    
</div>


  

  
    


</article>


        </div>
        
    


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-132808247-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>


<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
        extensions: ["AMSmath.js", "AMSsymbols.js"]
      }
    }
  });
  MathJax.Hub.Queue(function () {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
    
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

    
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    





<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.js"></script>
<script type="text/javascript">
  if (tocbot) {
    tocbot.init({
      
      tocSelector: '.toc',
      
      contentSelector: '.post',
      
      headingSelector: 'h2, h3, h4',
      collapseDepth: 4
    });
  }
</script>


    



    </body>
</html>
